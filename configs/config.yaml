# Task / Model
dim: 100
ntokens: 100
rank: 100
nheads: 1

# Training / Logging
batch_size: 256
lr: 1e-2
epochs: 400
num_workers: 16
experiment_name: ???
experiment_version: ???
git_hash: ???
skip_wandb: ${debug}
debug: False

code_dir: /home/nia4240/attention-formers
csv_log_dir: ${code_dir}/csv_logs
wandb_log_parent_dir: ${code_dir}
slurm_log_dir: ${code_dir}/slurm_logs

overlay: /scratch/nia4240/overlay-50G-10M.ext3
conda_env: attention

slurm:
  open_mode: append
  output: ${slurm_log_dir}/%A_%a.out
  error: ${slurm_log_dir}/%A_%a.err
  time: 0:15:00
  mem: 64G
  cpus_per_task: ${num_workers}
  gpus_per_node: 1
