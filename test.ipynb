{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Module, ModuleList, Linear, LayerNorm, MultiheadAttention\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyMultiheadAttention(Module):\n",
    "#     # start from here: https://nn.labml.ai/transformers/mha.html#section-5\n",
    "\n",
    "class LinearformerLayer(Module):\n",
    "    # TODO: extremely frustrating but they require the dimension of the token representations to be divisible by the number of heads\n",
    "    def __init__(self, d_model, nhead,\n",
    "                 batch_first=True, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, bias=False, batch_first=batch_first, **factory_kwargs)\n",
    "        self.linear = Linear(d_model, d_model, bias=False, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N.B.: This assumes that all sequences in the batch have the same length\n",
    "        # if they are not, we will need to use `key_padding_mask`\n",
    "        x = self.self_attn(x, x, x, need_weights=False)[0]\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class LinearFormer(Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, \n",
    "                 batch_first=True, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(*(LinearformerLayer(\n",
    "            d_model, nhead, batch_first=batch_first, device=device, dtype=dtype) for _ in range(num_layers)))\n",
    "        # self.layers = ModuleList([LinearformerLayer(\n",
    "        #     d_model, nhead, batch_first=batch_first, device=device, dtype=dtype) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    @classmethod\n",
    "    def farthest(cls, d_model, batch_first=True, device=None, dtype=None):\n",
    "        model = cls(1, d_model, 1, batch_first=batch_first, device=device, dtype=dtype)\n",
    "        model.layers[0].linear.weight.data = torch.eye(*model.layers[0].linear.weight.data.shape)\n",
    "        if model.layers[0].linear.bias is not None:\n",
    "            model.layers[0].linear.bias.data = torch.zeros_like(model.layers[0].linear.bias)\n",
    "        model.layers[0].self_attn.in_proj_weight.data = torch.concat([torch.eye(d_model), -1000 * torch.eye(d_model), torch.eye(d_model)])\n",
    "        if model.layers[0].self_attn.in_proj_bias is not None:\n",
    "            model.layers[0].self_attn.in_proj_bias.data = torch.zeros_like(model.layers[0].self_attn.in_proj_bias)\n",
    "        model.layers[0].self_attn.out_proj.weight.data = torch.eye(*model.layers[0].self_attn.out_proj.weight.shape)\n",
    "        if model.layers[0].self_attn.out_proj.bias:\n",
    "            model.layers[0].self_attn.out_proj.bias.data = torch.zeros_like(model.layers[0].self_attn.out_proj.bias)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def farthest_perturbed(cls, d_model, batch_first=True, device=None, dtype=None):\n",
    "        scale = .01\n",
    "        model = cls(1, d_model, 1, batch_first=batch_first, device=device, dtype=dtype)\n",
    "        model.layers[0].linear.weight.data = torch.eye(*model.layers[0].linear.weight.data.shape) + scale * torch.randn_like(model.layers[0].linear.weight.data)\n",
    "        if model.layers[0].linear.bias is not None:\n",
    "            model.layers[0].linear.bias.data = torch.zeros_like(model.layers[0].linear.bias) + scale * torch.randn_like(model.layers[0].linear.bias)\n",
    "        model.layers[0].self_attn.in_proj_weight.data = torch.concat([torch.eye(d_model), -1000 * torch.eye(d_model), torch.eye(d_model)]) + scale * torch.randn_like(model.layers[0].self_attn.in_proj_weight.data)\n",
    "        if model.layers[0].self_attn.in_proj_bias is not None:\n",
    "            model.layers[0].self_attn.in_proj_bias.data = torch.zeros_like(model.layers[0].self_attn.in_proj_bias) + scale * torch.randn_like(model.layers[0].self_attn.in_proj_bias.data)\n",
    "        model.layers[0].self_attn.out_proj.weight.data = torch.eye(*model.layers[0].self_attn.out_proj.weight.shape) + scale * torch.randn_like(model.layers[0].self_attn.out_proj.weight.data)\n",
    "        if model.layers[0].self_attn.out_proj.bias:\n",
    "            model.layers[0].self_attn.out_proj.bias.data = torch.zeros_like(model.layers[0].self_attn.out_proj.bias) + scale * torch.randn_like(model.layers[0].self_attn.out_proj.bias.data)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence(ntokens, dim, rng=None):\n",
    "    x = torch.randn((ntokens, dim), generator=rng)\n",
    "    # normalize each row\n",
    "    return x / x.norm(dim=1).reshape(-1, 1)\n",
    "\n",
    "def label_farthest(sentence):\n",
    "    distances = 2 - 2 * sentence @ sentence.T\n",
    "    farthests = distances.argmax(dim=0)\n",
    "    return sentence[farthests, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FarthestPointDataset(torch.utils.data.IterableDataset):\n",
    "    # TODO: let ntokens vary according to some reasonable distribution\n",
    "    def __init__(self, ntokens, dim, seed=None):\n",
    "        super().__init__()\n",
    "        self.ntokens = ntokens\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = torch.Generator()\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        worker_id = 0 if worker_info is None else worker_info.id\n",
    "        seed = rng.seed() if self.seed is None else self.seed\n",
    "        rng.manual_seed(seed + worker_id)\n",
    "\n",
    "        while True:\n",
    "            sentence = gen_sentence(self.ntokens, self.dim, rng=rng)\n",
    "            yield sentence, label_farthest(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSequenceRegression(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # return optimizer\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9, verbose=False)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 10\n",
    "dim = 4\n",
    "num_layers = 1\n",
    "nhead = 1\n",
    "batch_size = 64\n",
    "# lit_model = LitSequenceRegression(LinearFormer(num_layers, dim, nhead))\n",
    "lit_model = LitSequenceRegression(LinearFormer.farthest_perturbed(dim))\n",
    "data = torch.utils.data.DataLoader(FarthestPointDataset(ntokens, dim), batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | LinearFormer | 80    \n",
      "---------------------------------------\n",
      "80        Trainable params\n",
      "0         Non-trainable params\n",
      "80        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:  93%|█████████▎| 93/100 [00:00<00:00, 111.39it/s, v_num=21, train_loss=0.000607] "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=200)\n",
    "trainer.fit(model=lit_model, train_dataloaders=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gen_sentence(ntokens, dim)\n",
    "y = label_farthest(x)\n",
    "yhat = lit_model.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0042, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(FarthestPointDataset(3, dim), batch_size=1000, num_workers=0)\n",
    "x, y = next(iter(dl))\n",
    "yhat = lit_model.model(x)\n",
    "torch.nn.functional.mse_loss(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0020, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x)\n",
    "# print(y)\n",
    "# print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self_attn.in_proj_weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-2.1683, -3.5842,  3.3000,  4.0539],\n",
       "          [ 1.3107, -3.1545,  3.1496, -4.6916],\n",
       "          [-4.1139, -3.4144, -3.9734, -1.6968],\n",
       "          [-4.5240,  3.5688,  2.6734, -1.8239],\n",
       "          [ 2.1421,  3.3045, -3.3916, -3.9073],\n",
       "          [-1.3476,  3.0847, -3.2302,  4.6493],\n",
       "          [ 3.8582,  2.9305,  3.7388,  1.6091],\n",
       "          [ 4.6814, -3.3468, -2.6973,  1.7620],\n",
       "          [-1.0091, -0.3342, -0.1035, -0.1046],\n",
       "          [ 0.0347,  0.8540,  0.6670, -0.7823],\n",
       "          [ 0.2932, -0.8154,  0.4150, -0.5697],\n",
       "          [-0.1577,  0.1001,  1.0149,  0.3995]], requires_grad=True)),\n",
       " ('self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1892, -0.2359, -0.6602,  0.9116],\n",
       "          [ 0.8456,  0.3112, -0.2822,  0.0273],\n",
       "          [-0.2101,  0.8255,  0.0537,  0.3716],\n",
       "          [ 0.3103, -0.1808,  0.6831,  0.4201]], requires_grad=True)),\n",
       " ('linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0961, -1.0117,  0.1761, -0.1656],\n",
       "          [ 0.1407,  0.2150,  0.3745, -0.7991],\n",
       "          [ 0.3342, -0.0847,  0.4912,  0.6764],\n",
       "          [ 0.8157, -0.2313, -0.4356,  0.0187]], requires_grad=True))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0 = lit_model.model.layers[0]\n",
    "# layer0 = LinearFormer.farthest(dim).layers[0]\n",
    "list(layer0.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0091, -0.3342, -0.1035, -0.1046],\n",
       "        [ 0.0347,  0.8540,  0.6670, -0.7823],\n",
       "        [ 0.2932, -0.8154,  0.4150, -0.5697],\n",
       "        [-0.1577,  0.1001,  1.0149,  0.3995]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0.self_attn.in_proj_weight[-dim:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5772,  0.2941,  0.7836,  0.4064],\n",
       "        [-0.1744, -0.0619,  0.0508, -1.3302],\n",
       "        [ 0.1342,  1.0381, -0.3636, -0.1514],\n",
       "        [ 0.5791,  0.0262,  0.3389, -0.3902]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0.self_attn.in_proj_weight[-dim:,:] @ layer0.self_attn.out_proj.weight @ layer0.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 31.8949, -22.1183,  20.3345,   4.2615],\n",
       "        [ -2.7532,  19.5320,  30.1750, -22.9863],\n",
       "        [-27.4843, -30.0926,  14.7030,  -9.1836],\n",
       "        [-12.7243,   9.9977,  18.7303,  35.3572]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0.self_attn.in_proj_weight[:dim,:] @ layer0.self_attn.in_proj_weight[dim:(2*dim),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x2 and 4x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m layer0\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49min_proj_weight[\u001b[39m4\u001b[39;49m:\u001b[39m8\u001b[39;49m,:] \u001b[39m@\u001b[39;49m layer0\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49min_proj_weight[:\u001b[39m4\u001b[39;49m,:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2 and 4x2)"
     ]
    }
   ],
   "source": [
    "layer0.self_attn.in_proj_weight[4:8,:] @ layer0.self_attn.in_proj_weight[:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[-0.5255, -0.8508],\n",
      "         [ 0.7689, -0.6394],\n",
      "         [ 0.1725,  0.9850]],\n",
      "\n",
      "        [[ 0.1269, -0.9919],\n",
      "         [ 0.0750,  0.9972],\n",
      "         [-0.8024, -0.5968]]])\n",
      "Label:\n",
      "tensor([[[ 0.1725,  0.9850],\n",
      "         [ 0.1725,  0.9850],\n",
      "         [-0.5255, -0.8508]],\n",
      "\n",
      "        [[ 0.0750,  0.9972],\n",
      "         [ 0.1269, -0.9919],\n",
      "         [ 0.0750,  0.9972]]])\n",
      "Prediction:\n",
      "tensor([[[ 0.1725,  0.9850],\n",
      "         [ 0.1725,  0.9850],\n",
      "         [-0.5255, -0.8508]],\n",
      "\n",
      "        [[ 0.0750,  0.9972],\n",
      "         [ 0.1269, -0.9919],\n",
      "         [ 0.0750,  0.9972]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(FarthestPointDataset(3, 2), batch_size=2, num_workers=0)\n",
    "batch = next(iter(dl))\n",
    "print(\"Input:\")\n",
    "print(batch[0])\n",
    "print(\"Label:\")\n",
    "print(batch[1])\n",
    "print(\"Prediction:\")\n",
    "print(LinearFormer.farthest(2)(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
