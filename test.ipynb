{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Module, ModuleList, Linear, LayerNorm, MultiheadAttention\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyMultiheadAttention(Module):\n",
    "#     # start from here: https://nn.labml.ai/transformers/mha.html#section-5\n",
    "\n",
    "class LinearformerLayer(Module):\n",
    "    # TODO: extremely frustrating but they require the dimension of the token representations to be divisible by the number of heads\n",
    "    def __init__(self, d_model, nhead,\n",
    "                 batch_first=True, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, batch_first=batch_first, **factory_kwargs)\n",
    "        self.linear = Linear(d_model, d_model, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N.B.: This assumes that all sequences in the batch have the same length\n",
    "        # if they are not, we will need to use `key_padding_mask`\n",
    "        x = self.self_attn(x, x, x, need_weights=False)[0]\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class LinearFormer(Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, \n",
    "                 batch_first=True, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(*(LinearformerLayer(\n",
    "            d_model, nhead, batch_first=batch_first, device=device, dtype=dtype) for _ in range(num_layers)))\n",
    "        # self.layers = ModuleList([LinearformerLayer(\n",
    "        #     d_model, nhead, batch_first=batch_first, device=device, dtype=dtype) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    @classmethod\n",
    "    def farthest(cls, d_model, batch_first=True, device=None, dtype=None):\n",
    "        model = cls(1, d_model, 1, batch_first=batch_first, device=device, dtype=dtype)\n",
    "        model.layers[0].linear.weight.data = torch.eye(*model.layers[0].linear.weight.data.shape)\n",
    "        model.layers[0].linear.bias.data = torch.zeros_like(model.layers[0].linear.bias)\n",
    "        model.layers[0].self_attn.in_proj_weight.data = torch.concat([torch.eye(2), -1000 * torch.eye(2), torch.eye(2)])\n",
    "        model.layers[0].self_attn.in_proj_bias.data = torch.zeros_like(model.layers[0].self_attn.in_proj_bias)\n",
    "        model.layers[0].self_attn.out_proj.weight.data = torch.eye(*model.layers[0].self_attn.out_proj.weight.shape)\n",
    "        model.layers[0].self_attn.out_proj.bias.data = torch.zeros_like(model.layers[0].self_attn.out_proj.bias)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence(ntokens, dim, rng=None):\n",
    "    x = torch.randn((ntokens, dim), generator=rng)\n",
    "    # normalize each row\n",
    "    return x / x.norm(dim=1).reshape(-1, 1)\n",
    "\n",
    "def label_farthest(sentence):\n",
    "    distances = 2 - 2 * sentence @ sentence.T\n",
    "    farthests = distances.argmax(dim=0)\n",
    "    return sentence[farthests, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FarthestPointDataset(torch.utils.data.IterableDataset):\n",
    "    # TODO: let ntokens vary according to some reasonable distribution\n",
    "    def __init__(self, ntokens, dim, seed=None):\n",
    "        super().__init__()\n",
    "        self.ntokens = ntokens\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = torch.Generator()\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        worker_id = 0 if worker_info is None else worker_info.id\n",
    "        seed = rng.seed() if self.seed is None else self.seed\n",
    "        rng.manual_seed(seed + worker_id)\n",
    "\n",
    "        while True:\n",
    "            sentence = gen_sentence(self.ntokens, self.dim, rng=rng)\n",
    "            yield sentence, label_farthest(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSequenceRegression(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.95, verbose=False)\n",
    "        # return optimizer\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 5\n",
    "dim = 4\n",
    "num_layers = 1\n",
    "nhead = 1\n",
    "batch_size = 64\n",
    "lit_model = LitSequenceRegression(LinearFormer(num_layers, dim, nhead))\n",
    "data = torch.utils.data.DataLoader(FarthestPointDataset(ntokens, dim), batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=100)\n",
    "trainer.fit(model=lit_model, train_dataloaders=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gen_sentence(ntokens, dim)\n",
    "y = label_farthest(x)\n",
    "yhat = lit_model.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[-0.5491, -0.8358],\n",
      "         [-0.9887, -0.1501],\n",
      "         [-0.4044, -0.9146]],\n",
      "\n",
      "        [[ 0.7892,  0.6141],\n",
      "         [-0.2937,  0.9559],\n",
      "         [-0.9933,  0.1159]]]), tensor([[[-0.9887, -0.1501],\n",
      "         [-0.4044, -0.9146],\n",
      "         [-0.9887, -0.1501]],\n",
      "\n",
      "        [[-0.9933,  0.1159],\n",
      "         [ 0.7892,  0.6141],\n",
      "         [ 0.7892,  0.6141]]])]\n",
      "tensor([[[-0.9887, -0.1501],\n",
      "         [-0.4044, -0.9146],\n",
      "         [-0.9887, -0.1501]],\n",
      "\n",
      "        [[-0.9933,  0.1159],\n",
      "         [ 0.7892,  0.6141],\n",
      "         [ 0.7892,  0.6141]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(FarthestPointDataset(3, 2), batch_size=2, num_workers=0)\n",
    "batch = next(iter(dl))\n",
    "print(batch)\n",
    "print(LinearFormer.farthest(2)(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
