{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Module, ModuleList, Linear, LayerNorm, MultiheadAttention\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyMultiheadAttention(Module):\n",
    "#     # start from here: https://nn.labml.ai/transformers/mha.html#section-5\n",
    "\n",
    "class LinearformerLayer(Module):\n",
    "    # TODO: extremely frustrating but they require the dimension of the token representations to be divisible by the number of heads\n",
    "    def __init__(self, d_model, nhead,\n",
    "                 batch_first=True, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, bias=False, batch_first=batch_first, **factory_kwargs)\n",
    "        self.linear = Linear(d_model, d_model, bias=False, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N.B.: This assumes that all sequences in the batch have the same length\n",
    "        # if they are not, we will need to use `key_padding_mask`\n",
    "        x = self.self_attn(x, x, x, need_weights=False)[0]\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class LinearFormer(Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, \n",
    "                 batch_first=True, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(*(LinearformerLayer(\n",
    "            d_model, nhead, batch_first=batch_first, device=device, dtype=dtype) for _ in range(num_layers)))\n",
    "        # self.layers = ModuleList([LinearformerLayer(\n",
    "        #     d_model, nhead, batch_first=batch_first, device=device, dtype=dtype) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    @classmethod\n",
    "    def farthest(cls, d_model, batch_first=True, device=None, dtype=None):\n",
    "        model = cls(1, d_model, 1, batch_first=batch_first, device=device, dtype=dtype)\n",
    "        model.layers[0].linear.weight.data = torch.eye(*model.layers[0].linear.weight.data.shape)\n",
    "        if model.layers[0].linear.bias is not None:\n",
    "            model.layers[0].linear.bias.data = torch.zeros_like(model.layers[0].linear.bias)\n",
    "        model.layers[0].self_attn.in_proj_weight.data = torch.concat([torch.eye(d_model), -1000 * torch.eye(d_model), torch.eye(d_model)])\n",
    "        if model.layers[0].self_attn.in_proj_bias is not None:\n",
    "            model.layers[0].self_attn.in_proj_bias.data = torch.zeros_like(model.layers[0].self_attn.in_proj_bias)\n",
    "        model.layers[0].self_attn.out_proj.weight.data = torch.eye(*model.layers[0].self_attn.out_proj.weight.shape)\n",
    "        if model.layers[0].self_attn.out_proj.bias:\n",
    "            model.layers[0].self_attn.out_proj.bias.data = torch.zeros_like(model.layers[0].self_attn.out_proj.bias)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def farthest_perturbed(cls, d_model, batch_first=True, device=None, dtype=None):\n",
    "        scale = .01\n",
    "        model = cls(1, d_model, 1, batch_first=batch_first, device=device, dtype=dtype)\n",
    "        model.layers[0].linear.weight.data = torch.eye(*model.layers[0].linear.weight.data.shape) + scale * torch.randn_like(model.layers[0].linear.weight.data)\n",
    "        if model.layers[0].linear.bias is not None:\n",
    "            model.layers[0].linear.bias.data = torch.zeros_like(model.layers[0].linear.bias) + scale * torch.randn_like(model.layers[0].linear.bias)\n",
    "        model.layers[0].self_attn.in_proj_weight.data = torch.concat([torch.eye(d_model), -1000 * torch.eye(d_model), torch.eye(d_model)]) + scale * torch.randn_like(model.layers[0].self_attn.in_proj_weight.data)\n",
    "        if model.layers[0].self_attn.in_proj_bias is not None:\n",
    "            model.layers[0].self_attn.in_proj_bias.data = torch.zeros_like(model.layers[0].self_attn.in_proj_bias) + scale * torch.randn_like(model.layers[0].self_attn.in_proj_bias.data)\n",
    "        model.layers[0].self_attn.out_proj.weight.data = torch.eye(*model.layers[0].self_attn.out_proj.weight.shape) + scale * torch.randn_like(model.layers[0].self_attn.out_proj.weight.data)\n",
    "        if model.layers[0].self_attn.out_proj.bias:\n",
    "            model.layers[0].self_attn.out_proj.bias.data = torch.zeros_like(model.layers[0].self_attn.out_proj.bias) + scale * torch.randn_like(model.layers[0].self_attn.out_proj.bias.data)\n",
    "        return model\n",
    "\n",
    "class MergedLinearFormer(Module):\n",
    "    def __init__(self, d_model, batch_first=True, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.QK = torch.nn.Parameter(torch.randn((d_model, d_model), device=device, dtype=dtype))\n",
    "        self.VO = torch.nn.Parameter(torch.randn((d_model, d_model), device=device, dtype=dtype))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, ntokens, dim = x.shape\n",
    "        # attn_matrix is batch_size, ntokens, ntokens\n",
    "        attn_matrix = torch.nn.Softmax(dim=2)(torch.einsum(\"btd,de,bue->btu\", x, self.QK, x) / sqrt(dim))\n",
    "        return torch.einsum(\"btu,bud,de->bte\", attn_matrix, x, self.VO)\n",
    "\n",
    "    @classmethod\n",
    "    def extract_attn(cls, my_layer):\n",
    "        dim = layer0.self_attn.out_proj.weight.data.shape[0]\n",
    "        merged = cls(dim)\n",
    "        merged.QK.data = my_layer.self_attn.in_proj_weight.data[:dim, :].T @ my_layer.self_attn.in_proj_weight.data[dim:(2*dim), :]\n",
    "        merged.VO.data = my_layer.self_attn.in_proj_weight.data[(2*dim):, :].T @ my_layer.self_attn.out_proj.weight.data.T @ my_layer.linear.weight.T\n",
    "        return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence(ntokens, dim, rng=None):\n",
    "    x = torch.randn((ntokens, dim), generator=rng)\n",
    "    # normalize each row\n",
    "    return x / x.norm(dim=1).reshape(-1, 1)\n",
    "\n",
    "def label_farthest(sentence):\n",
    "    distances = 2 - 2 * sentence @ sentence.T\n",
    "    farthests = distances.argmax(dim=0)\n",
    "    return sentence[farthests, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FarthestPointDataset(torch.utils.data.IterableDataset):\n",
    "    # TODO: let ntokens vary according to some reasonable distribution\n",
    "    def __init__(self, ntokens, dim, seed=None):\n",
    "        super().__init__()\n",
    "        self.ntokens = ntokens\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = torch.Generator()\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        worker_id = 0 if worker_info is None else worker_info.id\n",
    "        seed = rng.seed() if self.seed is None else self.seed\n",
    "        rng.manual_seed(seed + worker_id)\n",
    "\n",
    "        while True:\n",
    "            sentence = gen_sentence(self.ntokens, self.dim, rng=rng)\n",
    "            yield sentence, label_farthest(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSequenceRegression(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # return optimizer\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9, verbose=False)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 10\n",
    "dim = 4\n",
    "num_layers = 1\n",
    "nhead = 1\n",
    "batch_size = 64\n",
    "lit_model = LitSequenceRegression(LinearFormer(num_layers, dim, nhead))\n",
    "# lit_model = LitSequenceRegression(LinearFormer.farthest_perturbed(dim))\n",
    "# lit_model = LitSequenceRegression(LinearFormer.farthest(dim))\n",
    "data = torch.utils.data.DataLoader(FarthestPointDataset(ntokens, dim), batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(limit_train_batches=100, max_epochs=200)\n",
    "# trainer.fit(model=lit_model, train_dataloaders=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = gen_sentence(ntokens, dim)\n",
    "# y = label_farthest(x)\n",
    "# yhat = lit_model.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2501, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(FarthestPointDataset(ntokens=3, dim=4), batch_size=2, num_workers=0)\n",
    "x, y = next(iter(dl))\n",
    "yhat = lit_model.model(x)\n",
    "torch.nn.functional.mse_loss(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x)\n",
    "# print(y)\n",
    "# print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(lit_model.model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self_attn.in_proj_weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.3483,  0.1091,  0.1221,  0.0469],\n",
       "          [-0.1485, -0.4305,  0.5641,  0.3172],\n",
       "          [-0.2736,  0.3633, -0.5100, -0.4738],\n",
       "          [ 0.1057, -0.4365, -0.3454, -0.5276],\n",
       "          [ 0.3811,  0.1482,  0.3414, -0.3970],\n",
       "          [-0.2347,  0.5068,  0.1093, -0.1075],\n",
       "          [ 0.5156, -0.2980,  0.1174, -0.5813],\n",
       "          [-0.0958,  0.0225,  0.5258, -0.1198],\n",
       "          [ 0.2671, -0.2377,  0.5364, -0.4530],\n",
       "          [ 0.3074,  0.2341,  0.0942, -0.4606],\n",
       "          [ 0.4801,  0.5533,  0.3059,  0.1502],\n",
       "          [ 0.1991, -0.5952,  0.3031,  0.1864]], requires_grad=True)),\n",
       " ('self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.3476, -0.0949, -0.1037,  0.0528],\n",
       "          [-0.3409, -0.4293, -0.1437, -0.4244],\n",
       "          [-0.4102,  0.4311,  0.2849,  0.3513],\n",
       "          [ 0.0233,  0.2812,  0.0793,  0.1873]], requires_grad=True)),\n",
       " ('linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1073,  0.0315, -0.4665, -0.2328],\n",
       "          [-0.1369, -0.1204,  0.0856,  0.3005],\n",
       "          [-0.2629, -0.0591, -0.3165, -0.4898],\n",
       "          [ 0.0760,  0.2915, -0.4443,  0.0468]], requires_grad=True))]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0 = lit_model.model.layers[0]\n",
    "# layer0 = LinearFormer.farthest(dim).layers[0]\n",
    "list(layer0.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0518, -0.0260,  0.0318,  0.0421],\n",
       "         [ 0.0749, -0.0587,  0.0354,  0.0845],\n",
       "         [ 0.0768, -0.0613,  0.0359,  0.0879]],\n",
       "\n",
       "        [[ 0.1126, -0.0826,  0.0511,  0.1229],\n",
       "         [ 0.1121, -0.0826,  0.0499,  0.1222],\n",
       "         [ 0.1089, -0.0753,  0.0505,  0.1145]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MergedLinearFormer.extract_attn(layer0)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0518, -0.0260,  0.0318,  0.0421],\n",
       "         [ 0.0749, -0.0587,  0.0354,  0.0845],\n",
       "         [ 0.0768, -0.0613,  0.0359,  0.0879]],\n",
       "\n",
       "        [[ 0.1126, -0.0826,  0.0511,  0.1229],\n",
       "         [ 0.1121, -0.0826,  0.0499,  0.1222],\n",
       "         [ 0.1089, -0.0753,  0.0505,  0.1145]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0309, -0.1271, -0.0719, -0.1084],\n",
       "         [ 0.0206, -0.1124, -0.0644, -0.0963],\n",
       "         [ 0.0117, -0.0996, -0.0578, -0.0858]],\n",
       "\n",
       "        [[-0.0520, -0.0656, -0.0393, -0.0598],\n",
       "         [-0.0525, -0.0643, -0.0384, -0.0590],\n",
       "         [-0.0524, -0.0667, -0.0395, -0.0608]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tutorial = multihead_attn_eg.MultiHeadAttention(heads=1, d_model=4, dropout_prob=0, bias=False)\n",
    "tutorial.query.linear.weight.data = layer0.self_attn.in_proj_weight.data[:dim, :].T\n",
    "tutorial.key.linear.weight.data = layer0.self_attn.in_proj_weight.data[dim:(2*dim), :].T\n",
    "tutorial.value.linear.weight.data = layer0.self_attn.in_proj_weight.data[(2*dim):, :].T\n",
    "tutorial.output.weight.data = layer0.self_attn.out_proj.weight.data.T @ layer0.linear.weight.T\n",
    "tutorial.output.bias.data = torch.zeros(4)\n",
    "tutorial(query=x.swapaxes(0, 1), key=x.swapaxes(0, 1), value=x.swapaxes(0, 1)).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[-0.7552, -0.6555],\n",
      "         [ 0.6431, -0.7658],\n",
      "         [-0.1499, -0.9887]],\n",
      "\n",
      "        [[ 0.5727, -0.8197],\n",
      "         [-0.4568,  0.8895],\n",
      "         [-0.9479,  0.3185]]])\n",
      "Label:\n",
      "tensor([[[ 0.6431, -0.7658],\n",
      "         [-0.7552, -0.6555],\n",
      "         [ 0.6431, -0.7658]],\n",
      "\n",
      "        [[-0.4568,  0.8895],\n",
      "         [ 0.5727, -0.8197],\n",
      "         [ 0.5727, -0.8197]]])\n",
      "Prediction:\n",
      "tensor([[[ 0.6431, -0.7658],\n",
      "         [-0.7552, -0.6555],\n",
      "         [ 0.6431, -0.7658]],\n",
      "\n",
      "        [[-0.4568,  0.8895],\n",
      "         [ 0.5727, -0.8197],\n",
      "         [ 0.5727, -0.8197]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(FarthestPointDataset(3, 2), batch_size=2, num_workers=0)\n",
    "batch = next(iter(dl))\n",
    "print(\"Input:\")\n",
    "print(batch[0])\n",
    "print(\"Label:\")\n",
    "print(batch[1])\n",
    "print(\"Prediction:\")\n",
    "print(LinearFormer.farthest(2)(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multihead_attn_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial = multihead_attn_eg.MultiHeadAttention(heads=1, d_model=4, dropout_prob=0, bias=False)\n",
    "# tutorial.query.linear.weight.data = 10000 * torch.Tensor([\n",
    "#     [0,0,0,1],\n",
    "#     [0,0,0,0],\n",
    "#     [0,0,0,0],\n",
    "#     [0,0,0,0]\n",
    "# ]).T\n",
    "# tutorial.key.linear.weight.data = torch.eye(4)\n",
    "tutorial.query.linear.weight.data = 10000 * torch.Tensor([\n",
    "    [1,1,1,1],\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,0]\n",
    "]).T\n",
    "tutorial.key.linear.weight.data = torch.Tensor([\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,0],\n",
    "    [1,1,1,1]\n",
    "]).T\n",
    "tutorial.value.linear.weight.data = torch.eye(4)\n",
    "tutorial.output.weight.data = torch.eye(4)\n",
    "tutorial.output.bias.data = torch.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "inn[:, 0, :] @ self.query.linear.weight.data.T @ self.key.linear.weight.data @ inn[:, 0, :].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7544,  0.3092, -0.5718,  0.0915],\n",
       "         [-0.7544,  0.3092, -0.5718,  0.0915],\n",
       "         [-0.4610, -0.3171,  0.7305,  0.3915]],\n",
       "\n",
       "        [[-0.1032, -0.9727,  0.0969, -0.1838],\n",
       "         [-0.1032, -0.9727,  0.0969, -0.1838],\n",
       "         [-0.1032, -0.9727,  0.0969, -0.1838]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tutorial(query=x.swapaxes(0, 1), key=x.swapaxes(0, 1), value=x.swapaxes(0, 1)).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7544,  0.3092, -0.5718,  0.0915],\n",
       "         [-0.7544,  0.3092, -0.5718,  0.0915],\n",
       "         [-0.4610, -0.3171,  0.7305,  0.3915]],\n",
       "\n",
       "        [[-0.1032, -0.9727,  0.0969, -0.1838],\n",
       "         [-0.1032, -0.9727,  0.0969, -0.1838],\n",
       "         [-0.1032, -0.9727,  0.0969, -0.1838]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7544,  0.3092, -0.5718,  0.0915],\n",
       "         [-0.4610, -0.3171,  0.7305,  0.3915],\n",
       "         [-0.4610, -0.3171,  0.7305,  0.3915]],\n",
       "\n",
       "        [[-0.1032, -0.9727,  0.0969, -0.1838],\n",
       "         [-0.1032, -0.9727,  0.0969, -0.1838],\n",
       "         [-0.1032, -0.9727,  0.0969, -0.1838]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0.self_attn(x, x, x, need_weights=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7544,  0.3092, -0.5718,  0.0915],\n",
       "         [-0.4610, -0.3171,  0.7305,  0.3915],\n",
       "         [ 0.1082, -0.6548,  0.6668,  0.3390]],\n",
       "\n",
       "        [[-0.1032, -0.9727,  0.0969, -0.1838],\n",
       "         [-0.3567, -0.5130, -0.2228,  0.7483],\n",
       "         [-0.9331,  0.1338,  0.2648,  0.2032]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
